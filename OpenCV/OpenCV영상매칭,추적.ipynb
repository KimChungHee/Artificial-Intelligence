{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 영상매칭과 추적 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* (1) 키포인트는 영상의 특징이 있는 픽셀의 좌표와 그 주변 픽셀과의 관계에 대한 정보를 가진다.\n",
    "* (2) 매칭에 사용하기 위해서는 회전, 크기, 방향 등에 영향이 없어야 한다.\n",
    "* (3) 디스크립터는 키 포인트 주변 픽셀을 일정한 크기의 블록으로 나누어 키 포인트 주위의 밝기, 색상, 방향         등의 정보를 표현 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP \n",
    "    (1) 검출기 생성\n",
    "        - SIFT : 속도가 느린 단점 \n",
    "        - SURF : SIFT에서 성능 개선 \n",
    "        - ORB : 방향과 회전을 고려하도록 개선\n",
    "    (2) 찾아서 검출\n",
    "    (3) 매칭기 생성 \n",
    "        - BRFMatcher : 전수 조사하는 알고리즘으로 영상이 큰 경우 속도가 느려짐  \n",
    "        - FLANN : 모든 특징 디스크립터를 비교하기보다 가장 가까운 이웃의 근사 값으로 매칭 \n",
    "    (4) 매칭\n",
    "    \n",
    "    (5) 좋은 매칭점 찾기 \n",
    "    : 가장 작은 거리 값과 큰 거리 값의 상위 몇 퍼센트만 골라서 좋은 매칭점으로 분류\n",
    "        - match()\n",
    "        - knnMatch() \n",
    "        \n",
    "    + 더할수 있는 것\n",
    "        (1) 매칭 영역 원근 변환 \n",
    "        : 좋은 매칭점으로만 구성된 매칭점을 좌표들로 두 영상 간의 원근 변환행렬을\n",
    "        구하면 찾는 물체가 영상 어디에 있는지 표시할 수 있다. \n",
    "        (2) 객체 추적 \n",
    "        (3) 옵티컬 플로 : 이전 장면과 다음 장면 사이의 픽셀이 이동한 방향과 거리에 대한 분포\n",
    "        => 영상 속 물체가 어느 방향으로 얼마만큼 움직였는지를 알 수 있다. \n",
    "        (4) MeanShift 추적 : 대상 객체의 색상 정보로 추적 \n",
    "        (5) CamShift 추적 : 고정된 윈도 크기와 방향을 개선 \n",
    "        (6) Tracking API : 추적할 객체의 초기 위치만 전달하면 객체 추적 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 공통함수 정의 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **keypoints, descriptors = detector.detectAndCompute(image, mask)**\n",
    ": 키 포인트 검출과 특징 디스크립터 계산을 한번에 수행\n",
    "    - image : 입력 영상\n",
    "    - mask: 키포인트 검출에 사용할 마스크\n",
    "    - keypoints : 디스크립터 계산을 위해 사용할 키 포인트\n",
    "    - descriptors : 계산된 디스크립터 \n",
    "      \n",
    "* **outImg = cv2.drawKeypoints(img, keypoints, outImg[,color[,flags]])** : 검출 포인트를 표시 \n",
    "    - img : 입력이미지\n",
    "    - keypoints : 표시할 키 포인트 리스트\n",
    "    - outImg : 키 포인트가 그려진 결과 이미지\n",
    "    - color : 표시할 색상(기본값 랜덤)\n",
    "    - flags : 표시 방법 선택 플래그 \n",
    "        - cv2.DRAW_MATCHES_FLAGS_DEFAULT : 좌표 중심에 동그라미만 그림\n",
    "        - cv2.DRAW_MATCHS_FLAGS_DRAW_RICH_KEYPOINTS : 동그라미의 크기를 size와 angle 반영"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SIFT\n",
    ": 크기 변화에 따른 특징 검출 문제를 해결하기 위해 이미지 피라미드를 사용하므로 속도가 느린 단점 개선"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* (1) detector = cv2.xfeatures2d.SIFT_create() : SIFT 검출기 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keypoint: 413 descriptor: (413, 128)\n",
      "[[  1.   1.   1. ...   0.   0.   1.]\n",
      " [  8.  24.   0. ...   1.   0.   4.]\n",
      " [  0.   0.   0. ...   0.   0.   2.]\n",
      " ...\n",
      " [  1.   8.  71. ...  73. 127.   3.]\n",
      " [ 35.   2.   7. ...   0.   0.   9.]\n",
      " [ 36.  34.   3. ...   0.   0.   1.]]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.imread('img/house.jpg')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# SIFT 추출기 생성\n",
    "sift = cv2.xfeatures2d.SIFT_create()\n",
    "# 키 포인트 검출과 서술자 계산(mask : None)\n",
    "keypoints, descriptor = sift.detectAndCompute(gray, None)\n",
    "print('keypoint:',len(keypoints), 'descriptor:', descriptor.shape)\n",
    "print(descriptor)\n",
    "\n",
    "# 키 포인트 그리기(outimg(x),동그라미의 크기를 size와 angle 반영)\n",
    "img_draw = cv2.drawKeypoints(img, keypoints, None, \\\n",
    "                flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "# 결과 출력\n",
    "cv2.imshow('SIFT', img_draw)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SURF\n",
    ": 크기 변화에 따른 특징 검출 문제를 해결하기 위해 이미지 피라미드를 사용하므로 속도가 느린 단점 개선"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* detector = cv2xfeatures2d.SURF_create([hessianThreshold, n0ctaves. n0taveLayers, extended, upright])\n",
    "    - hessianThreshold : 특징 추출 경계 값(100)\n",
    "    - n0cdtaves : 이미지 프라미드 계층 수 (3)\n",
    "    - extended : 디스크립터 생성 플래그(False), True : 128개, False : 64개 \n",
    "    - upright : 방향 계산 플래그(False), True : 방향 무시, False ; 방향적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(107, 128) [[ 8.9554851e-06  1.1385256e-04  5.4340991e-03 ...  2.9642240e-04\n",
      "  -3.7367863e-04  6.0976070e-04]\n",
      " [-1.4285169e-03  1.6189254e-03 -3.4004995e-03 ...  3.1197818e-03\n",
      "  -9.3057424e-05  1.8120672e-04]\n",
      " [ 5.7166762e-04  6.1247620e-04  3.0916829e-03 ...  8.3260617e-05\n",
      "   7.0488331e-04  7.0711551e-04]\n",
      " ...\n",
      " [ 2.8933402e-05  1.9248344e-04 -9.6321059e-04 ...  6.7522678e-06\n",
      "  -5.9432325e-05  7.8575125e-05]\n",
      " [ 1.4399580e-05  5.8301219e-05 -1.3863634e-04 ...  5.2285664e-05\n",
      "  -6.1131141e-04  6.3628476e-04]\n",
      " [-1.1766193e-03  1.2112194e-03 -1.1587985e-02 ...  1.2997333e-04\n",
      "  -2.8113127e-04  3.0559112e-04]]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.imread('img/house.jpg')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# SURF 추출기 생성 ( 경계:1000, 피라미드:5, 서술자확장:True, 방향적용:True)\n",
    "# 경계 값을 100으로, 피라미드 값을 3으로 변경해보기\n",
    "# => 더 많이 추출되고, 값이 커진다.\n",
    "surf = cv2.xfeatures2d.SURF_create(1000, 5, True, True)\n",
    "\n",
    "# 키 포인트 검출 및 서술자 계산(마스크 없음)\n",
    "keypoints, desc = surf.detectAndCompute(gray, None)\n",
    "print(desc.shape, desc)\n",
    "# 키포인트 이미지에 그리기 (outimg(x),동그라미의 크기를 size와 angle 반영)\n",
    "img_draw = cv2.drawKeypoints(img, keypoints, None, \\\n",
    "                flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "cv2.imshow('SURF', img_draw)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ORB \n",
    ": 방향과 회전을 고려하도록 개선한 알고리즘\n",
    "* dectector = cv.ORB_create() : 검출기 생성 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.imread('img/house.jpg')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# ORB 추출기 생성\n",
    "orb = cv2.ORB_create()\n",
    "# 키 포인트 검출과 서술자 계산\n",
    "keypoints, descriptor = orb.detectAndCompute(img, None)\n",
    "# 키 포인트 그리기\n",
    "img_draw = cv2.drawKeypoints(img, keypoints, None, \\\n",
    "             flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "# 결과 출력\n",
    "cv2.imshow('ORB', img_draw)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 특징매칭\n",
    ": 서로 다른 두 영상에서 구한 키 포인트와 특징 디스크립터들을 각각 비교해서 그 거리가 비슷한 것 끼리 짝짓는 것\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 공통함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* matches = matcher.match(queryDescriptors, trainDescriptors) :1개의 최적 매칭 \n",
    "    - queryDescriptors : 매칭 기준이 될 디스크립터 \n",
    "    - trainDescriptors : 매칭의 대상이 될 디스크립터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BFMatcher \n",
    "* matcher = cv2.BFMatcher_create(normType)\n",
    "    - normType : 거리 측정 알고리즘\n",
    "        - cv2.NORM_L1 : 절대값 차 (SIFT, SURF)\n",
    "        - cv2.NORM_L2 : 제곱합의 루트 (SIFT, SURF) , 기본값\n",
    "        - cv2.NORM_L2SQR : 제곱합 (SIFT, SURF)\n",
    "        - cv2.NORM_HAMMING (ORB)\n",
    "        - cv2.NORM_HAMMING2 (ORB)\n",
    "        \n",
    "    - corssCheck = False : 상호 매칭이 있는 것만 반영\n",
    "      (True로 설정할 경우 불필요한 매칭을 줄일 수 있으며 속도가 느려진다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BFMatcher와 SIFT로 매칭"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, numpy as np\n",
    "\n",
    "img1 = cv2.imread('img/taekwonv1.jpg')\n",
    "img2 = cv2.imread('img/figures.jpg')\n",
    "gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "## SIFT 서술자 추출기 생성 ---①\n",
    "detector = cv2.xfeatures2d.SIFT_create()\n",
    "# 각 영상에 대해 키 포인트와 서술자 추출 ---②\n",
    "kp1, desc1 = detector.detectAndCompute(gray1, None)\n",
    "kp2, desc2 = detector.detectAndCompute(gray2, None)\n",
    "\n",
    "# BFMatcher 생성, L1 거리, 상호 체크 ---③\n",
    "matcher = cv2.BFMatcher(cv2.NORM_L1, crossCheck=True)\n",
    "# 매칭 계산 ---④\n",
    "matches = matcher.match(desc1, desc2)\n",
    "# 매칭 결과 그리기 ---⑤\n",
    "res = cv2.drawMatches(img1, kp1, img2, kp2, matches, None, \\\n",
    "                      flags=cv2.DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS)\n",
    "# 결과 출력 \n",
    "cv2.imshow('BFMatcher + SIFT', res)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BFMatcher와 SURF로 매칭"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img1 = cv2.imread('img/taekwonv1.jpg')\n",
    "img2 = cv2.imread('img/figures.jpg')\n",
    "gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# SURF 서술자 추출기 생성 ---①\n",
    "detector = cv2.xfeatures2d.SURF_create()\n",
    "kp1, desc1 = detector.detectAndCompute(gray1, None)\n",
    "kp2, desc2 = detector.detectAndCompute(gray2, None)\n",
    "\n",
    "# BFMatcher 생성, L2 거리, 상호 체크 ---③\n",
    "matcher = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n",
    "# 매칭 계산 ---④\n",
    "matches = matcher.match(desc1, desc2)\n",
    "# 매칭 결과 그리기 ---⑤\n",
    "res = cv2.drawMatches(img1, kp1, img2, kp2, matches, None, \\\n",
    "                     flags=cv2.DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS)\n",
    "\n",
    "cv2.imshow('BF + SURF', res)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BFMatcher와 ORB로 매칭 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, numpy as np\n",
    "\n",
    "img1 = cv2.imread('img/taekwonv1.jpg')\n",
    "img2 = cv2.imread('img/figures.jpg')\n",
    "gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# SIFT 서술자 추출기 생성 ---①\n",
    "detector = cv2.ORB_create()\n",
    "# 각 영상에 대해 키 포인트와 서술자 추출 ---②\n",
    "kp1, desc1 = detector.detectAndCompute(gray1, None)\n",
    "kp2, desc2 = detector.detectAndCompute(gray2, None)\n",
    "\n",
    "# BFMatcher 생성, Hamming 거리, 상호 체크 ---③\n",
    "matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "# 매칭 계산 ---④\n",
    "matches = matcher.match(desc1, desc2)\n",
    "# 매칭 결과 그리기 ---⑤\n",
    "res = cv2.drawMatches(img1, kp1, img2, kp2, matches, None, \\\n",
    "                     flags=cv2.DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS)\n",
    "\n",
    "cv2.imshow('BFMatcher + ORB', res)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FLANN\n",
    ": BFMatcher는 특징 디스크립터를 전수 조사 하므로 매칭에 사용할 영상이 큰 경우 속도가 느려질때 사용\n",
    "\n",
    "인덱스 파라미터에 값을 조금만 잘못 지정해도 매칭에 실패하고 오류가 발생하여 OpenCV튜토리얼 문서에서 아래와 같이 인덱스 파라미터 설정을 권장\n",
    "\n",
    "    (1) SIFT or SURF\n",
    "        - FLANN_INDEX_KDTREE = 1\n",
    "        - index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
    "    \n",
    "    (2) ORB를 사용하는 경우 \n",
    "        FLANN_INDEX_LSH = 6\n",
    "        - index_params= dict(algorithm = FLANN_INDEX_LSH,\n",
    "                             table_number = 6,\n",
    "                             key_size = 12,\n",
    "                             multi_probe_level = 1)\n",
    "#####    \n",
    "* matcher = cv2.FlannBasedMatcher(indexParams, searchParams)\n",
    "    - indexParams : 인섹스 파라미터, 딕셔너리\n",
    "    - searchParam : 검색할 파라미터, 딕셔너리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, numpy as np\n",
    "\n",
    "img1 = cv2.imread('img/taekwonv1.jpg')\n",
    "img2 = cv2.imread('img/figures.jpg')\n",
    "gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# SIFT 생성\n",
    "detector = cv2.xfeatures2d.SIFT_create()\n",
    "# 키 포인트와 서술자 추출\n",
    "kp1, desc1 = detector.detectAndCompute(gray1, None)\n",
    "kp2, desc2 = detector.detectAndCompute(gray2, None)\n",
    "\n",
    "# 인덱스 파라미터와 검색 파라미터 설정 ---①\n",
    "FLANN_INDEX_KDTREE = 1\n",
    "index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
    "search_params = dict(checks=50) # 검색 파라미터 , 딕셔너리 객체(checks = 검색할 후보수)\n",
    "\n",
    "# Flann 매처 생성 ---③\n",
    "matcher = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "# 매칭 계산 ---④\n",
    "matches = matcher.match(desc1, desc2)\n",
    "# 매칭 그리기\n",
    "res = cv2.drawMatches(img1, kp1, img2, kp2, matches, None, \\\n",
    "                flags=cv2.DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS)\n",
    "\n",
    "cv2.imshow('Flann + SIFT', res)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FLANNMatcher와 SURF로 매칭"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, numpy as np\n",
    "\n",
    "img1 = cv2.imread('img/taekwonv1.jpg')\n",
    "img2 = cv2.imread('img/figures.jpg')\n",
    "gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# SURF 생성\n",
    "detector = cv2.xfeatures2d.SURF_create()\n",
    "# 키 포인트와 서술자 추출\n",
    "kp1, desc1 = detector.detectAndCompute(gray1, None)\n",
    "kp2, desc2 = detector.detectAndCompute(gray2, None)\n",
    "\n",
    "# 인덱스 파라미터와 검색 파라미터 설정 ---①\n",
    "FLANN_INDEX_KDTREE = 1\n",
    "index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
    "search_params = dict(checks=50)\n",
    "\n",
    "# Flann 매처 생성 ---③\n",
    "matcher = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "# 매칭 계산 ---④\n",
    "matches = matcher.match(desc1, desc2)\n",
    "# 매칭 그리기\n",
    "res = cv2.drawMatches(img1, kp1, img2, kp2, matches, None, \\\n",
    "                flags=cv2.DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS)\n",
    "\n",
    "cv2.imshow('Flann + SURF', res)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FLANNMatcher와 ORB로 매칭 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, numpy as np\n",
    "\n",
    "img1 = cv2.imread('img/taekwonv1.jpg')\n",
    "img2 = cv2.imread('img/figures.jpg')\n",
    "gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# ORB 추출기 생성\n",
    "detector = cv2.ORB_create()\n",
    "# 키 포인트와 서술자 추출\n",
    "kp1, desc1 = detector.detectAndCompute(gray1, None)\n",
    "kp2, desc2 = detector.detectAndCompute(gray2, None)\n",
    "\n",
    "# 인덱스 파라미터 설정 ---①\n",
    "FLANN_INDEX_LSH = 6\n",
    "index_params= dict(algorithm = FLANN_INDEX_LSH,\n",
    "                   table_number = 6,\n",
    "                   key_size = 12,\n",
    "                   multi_probe_level = 1)\n",
    "# 검색 파라미터 설정 ---②\n",
    "search_params=dict(checks=32)  # 검색 파라미터 , 딕셔너리 객체(checks = 검색할 후보수)\n",
    "# Flann 매처 생성 ---③\n",
    "matcher = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "# 매칭 계산 ---④\n",
    "matches = matcher.match(desc1, desc2)\n",
    "# 매칭 그리기\n",
    "res = cv2.drawMatches(img1, kp1, img2, kp2, matches, None, \\\n",
    "            flags=cv2.DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS)\n",
    "# 결과 출력            \n",
    "cv2.imshow('Flann + ORB', res)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 공통함수\n",
    "* DMatch : 매칭 결과를 표현하는 객체 \n",
    "    - queryIdx : queryDescriptor의 인덱스 \n",
    "    - trainIdx : trainDescriptor의 인덱스\n",
    "    - imgIdx : 이미지 인덱스 \n",
    "    - distance : 유사도 거리 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 좋은 매칭점 찾기\n",
    ": 결과에서 쓸모 없는 매칭점은 버리고 좋은 매칭점만을 고랄내는 작업 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matches:30/240, min:25.00, max:92.00, thresh:38.40\n"
     ]
    }
   ],
   "source": [
    "import cv2, numpy as np\n",
    "\n",
    "img1 = cv2.imread('img/taekwonv1.jpg')\n",
    "img2 = cv2.imread('img/figures.jpg')\n",
    "gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# ORB로 서술자 추출 ---①\n",
    "detector = cv2.ORB_create()\n",
    "kp1, desc1 = detector.detectAndCompute(gray1, None)\n",
    "kp2, desc2 = detector.detectAndCompute(gray2, None)\n",
    "# BF-Hamming으로 매칭 ---②\n",
    "matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "matches = matcher.match(desc1, desc2)\n",
    "\n",
    "# 매칭 결과를 거리기준 오름차순으로 정렬 ---③\n",
    "matches = sorted(matches, key=lambda x:x.distance)\n",
    "\n",
    "# 최소 거리 값과 최대 거리 값 확보 ---④\n",
    "min_dist, max_dist = matches[0].distance, matches[-1].distance\n",
    "# 최소 거리의 20% 지점을 임계점으로 설정 ---⑤\n",
    "ratio = 0.2\n",
    "good_thresh = (max_dist - min_dist) * ratio + min_dist\n",
    "\n",
    "# 임계점 보다 작은 매칭점만 좋은 매칭점으로 분류 ---⑥\n",
    "#m.distance 매칭객체의 거리 공통함수부분참조  \n",
    "good_matches = [m for m in matches if m.distance < good_thresh]\n",
    "\n",
    "# 아래의 코드는 위의 한줄코드를 풀어 쓴것\n",
    "# good_matches = []\n",
    "# for m in matches:\n",
    "#     if m.distance < good_thresh:\n",
    "#         good_matches.append(m)\n",
    "\n",
    "print('matches:%d/%d, min:%.2f, max:%.2f, thresh:%.2f' \\\n",
    "        %(len(good_matches),len(matches), min_dist, max_dist, good_thresh))\n",
    "# 좋은 매칭점만 그리기 ---⑦\n",
    "res = cv2.drawMatches(img1, kp1, img2, kp2, good_matches, None, \\\n",
    "                flags=cv2.DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS)\n",
    "# 결과 출력\n",
    "cv2.imshow('Good Match', res)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### knnMatch 함수로 부터 좋은 매칭점 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matches:26/500\n"
     ]
    }
   ],
   "source": [
    "import cv2, numpy as np\n",
    "\n",
    "img1 = cv2.imread('img/taekwonv1.jpg')\n",
    "img2 = cv2.imread('img/figures.jpg')\n",
    "gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# ORB로 서술자 추출 ---①\n",
    "detector = cv2.ORB_create()\n",
    "kp1, desc1 = detector.detectAndCompute(gray1, None)\n",
    "kp2, desc2 = detector.detectAndCompute(gray2, None)\n",
    "# BF-Hamming 생성 ---②\n",
    "matcher = cv2.BFMatcher(cv2.NORM_HAMMING2)\n",
    "# knnMatch, k=2 ---③\n",
    "matches = matcher.knnMatch(desc1, desc2, 2)\n",
    "\n",
    "# 첫번재 이웃의 거리가 두 번째 이웃 거리의 75% 이내인 것만 추출---④\n",
    "# 근처에도 매칭 된 것이 있어야 매칭된다는 의미\n",
    "ratio = 0.75\n",
    "good_matches = [first for first,second in matches \\\n",
    "                    if first.distance < second.distance * ratio]\n",
    "\n",
    "#위의 코드 풀어쓰기\n",
    "# good_matches = []\n",
    "# for first,second in matches:\n",
    "#     if first.distance < second.distance * ratio:\n",
    "#         good_matches.append(first)\n",
    "\n",
    "print('matches:%d/%d' %(len(good_matches),len(matches)))\n",
    "\n",
    "# 좋은 매칭만 그리기\n",
    "res = cv2.drawMatches(img1, kp1, img2, kp2, good_matches, None, \\\n",
    "                    flags=cv2.DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS)\n",
    "# 결과 출력                    \n",
    "cv2.imshow('Matching', res)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 매칭 영역 원근 변환\n",
    "    (1) 좋은 매칭점으로만 구성된 매칭점 좌표들로 두 영상 간의 원근 변환행렬을 구하면 찾는 물체가 영상어디에 있는지 표시할 수 있습니다.\n",
    "    \n",
    "    (2) 원근 변환행렬에 맞지 않는 매칭점을 구분할 수 있어 나쁜 매칭점을 또 한번 제거\n",
    "\n",
    "* mtrx, mask = cv.findHomograpy(srcPoints, dstPoints)\n",
    ": 여러개의 점으로 근사 계산한 원근 변환행렬을 반환\n",
    "    - srcPoint : 원본 좌표 배열\n",
    "    - dstPoint : 결과 좌표 배열 \n",
    "    - mtrx : 결과 행렬\n",
    "    - mask : 정상치 판별결과, N X 1행 배열 ( 0 : 비정상치, 1: 정상치)\n",
    "* dst = cv.perspectiveTransform(src, m)\n",
    ": 이동할 새로운 좌표배열을 반환\n",
    "    - src : 입력 좌표 배열\n",
    "    - m : 변환 행렬\n",
    "    - dst : 출력 좌표 배열"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good matches:26/500\n"
     ]
    }
   ],
   "source": [
    "import cv2, numpy as np\n",
    "\n",
    "img1 = cv2.imread('img/taekwonv1.jpg')\n",
    "img2 = cv2.imread('img/figures.jpg')\n",
    "gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# ORB, BF-Hamming 로 knnMatch  ---①\n",
    "detector = cv2.ORB_create()\n",
    "kp1, desc1 = detector.detectAndCompute(gray1, None)\n",
    "kp2, desc2 = detector.detectAndCompute(gray2, None)\n",
    "matcher = cv2.BFMatcher(cv2.NORM_HAMMING2)\n",
    "matches = matcher.knnMatch(desc1, desc2, 2)\n",
    "\n",
    "# 이웃 거리의 75%로 좋은 매칭점 추출---②\n",
    "ratio = 0.75\n",
    "good_matches = [first for first,second in matches \\\n",
    "                    if first.distance < second.distance * ratio]\n",
    "print('good matches:%d/%d' %(len(good_matches),len(matches)))\n",
    "\n",
    "# 좋은 매칭점의 queryIdx로 원본 영상의 좌표 구하기 ---③\n",
    "src_pts = np.float32([ kp1[m.queryIdx].pt for m in good_matches ])\n",
    "\n",
    "#위의 코드가 이해 안되면 출력해보기\n",
    "#print(kp1[m.queryIdx]) #객체\n",
    "#print(kp1[m.queryIdx].pt) #좌표\n",
    "    \n",
    "# 좋은 매칭점의 trainIdx로 대상 영상의 좌표 구하기 ---④\n",
    "dst_pts = np.float32([ kp2[m.trainIdx].pt for m in good_matches ])\n",
    "# 원근 변환 행렬 구하기 ---⑤\n",
    "mtrx, mask = cv2.findHomography(src_pts, dst_pts)\n",
    "# 원본 영상 크기로 변환 영역 좌표 생성 ---⑥\n",
    "h,w, = img1.shape[:2]\n",
    "pts = np.float32([ [[0,0]],[[0,h-1]],[[w-1,h-1]],[[w-1,0]] ])\n",
    "# 원본 영상 좌표를 원근 변환  ---⑦\n",
    "dst = cv2.perspectiveTransform(pts,mtrx)\n",
    "# 변환 좌표 영역을 대상 영상에 그리기 ---⑧\n",
    "img2 = cv2.polylines(img2,[np.int32(dst)],True,255,3, cv2.LINE_AA)\n",
    "\n",
    "# 좋은 매칭 그려서 출력 ---⑨\n",
    "res = cv2.drawMatches(img1, kp1, img2, kp2, good_matches, None, \\\n",
    "                    flags=cv2.DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS)\n",
    "cv2.imshow('Matching Homography', res)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 팀예제) 파노라마 사진 생성\n",
    "## [Hint]\n",
    "### (1) 좌측 사진을 우측사진에 매칭\n",
    "### (2) 원근변환 행렬을 구하여, cv2.warpPerspective() 함수로 원근 변환 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, numpy as np\n",
    "\n",
    "# 왼쪽 오른쪽 사진 읽기\n",
    "imgL = cv2.imread('img/restaurant1.jpg') # train\n",
    "imgR = cv2.imread('img/restaurant2.jpg') # query\n",
    "hl, wl = imgL.shape[:2]\n",
    "hr, wr = imgR.shape[:2]\n",
    "\n",
    "grayL = cv2.cvtColor(imgL, cv2.COLOR_BGR2GRAY)\n",
    "grayR = cv2.cvtColor(imgR, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# SIFT 특징 검출기 생성 및 특징점 검출\n",
    "descriptor = cv2.xfeatures2d.SIFT_create()\n",
    "(kpsL, featuresL) = descriptor.detectAndCompute(imgL, None)\n",
    "(kpsR, featuresR) = descriptor.detectAndCompute(imgR, None)\n",
    "# BF 매칭기 생성 및 knn매칭\n",
    "matcher = cv2.DescriptorMatcher_create(\"BruteForce\")\n",
    "matches = matcher.knnMatch(featuresR, featuresL, 2)\n",
    "\n",
    "# 좋은 매칭점 선별\n",
    "good_matches = []\n",
    "for m in matches:\n",
    "    if len(m) == 2 and m[0].distance < m[1].distance * 0.75:\n",
    "        good_matches.append(( m[0].trainIdx, m[0].queryIdx))\n",
    "\n",
    "# 좋은 매칭점이 4개 이상 원근 변환 행렬 구하기\n",
    "if len(good_matches) > 4:\n",
    "    ptsL = np.float32([kpsL[i].pt for (i, _) in good_matches])\n",
    "    ptsR = np.float32([kpsR[i].pt for (_, i) in good_matches])\n",
    "    mtrx, status = cv2.findHomography(ptsR,ptsL, cv2.RANSAC, 4.0)\n",
    "    #원근 변환 행렬로 오른쪽 사진을 원근 변환, 결과 이미지 크기는 사진 2장 크기\n",
    "    panorama = cv2.warpPerspective(imgR, mtrx, (wr + wl, hr))\n",
    "    # 왼쪽 사진을 원근 변환한 왼쪽 영역에 합성\n",
    "    panorama[0:hl, 0:wl] = imgL\n",
    "else:\n",
    "    panorama = imgL\n",
    "cv2.imshow(\"Image Left\", imgL)\n",
    "cv2.imshow(\"Image Right\", imgR)\n",
    "cv2.imshow(\"Panorama\", panorama)\n",
    "cv2.waitKey(0)                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 카메라로 객체 매칭\n",
    "    - 노트북있는 팀원에게 가서 같이 해보기\n",
    "\n",
    "    (1) 인식할 물체를 들고 스페이스바를 누른다. \n",
    "    (2) 마우스로 해당물체를 roi로 지정해준다.\n",
    "    (독특한 문양이나, 모양이 아닐경우 얼굴을 roi 지정해서 해보자)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can't open camera.\n"
     ]
    }
   ],
   "source": [
    "import cv2, numpy as np\n",
    "\n",
    "img1 = None\n",
    "win_name = 'Camera Matching'\n",
    "MIN_MATCH = 10\n",
    "# ORB 검출기 생성  ---①\n",
    "detector = cv2.ORB_create(1000)\n",
    "# Flann 추출기 생성 ---②\n",
    "FLANN_INDEX_LSH = 6\n",
    "index_params= dict(algorithm = FLANN_INDEX_LSH,\n",
    "                   table_number = 6,\n",
    "                   key_size = 12,\n",
    "                   multi_probe_level = 1)\n",
    "search_params=dict(checks=32)\n",
    "matcher = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "# 카메라 캡쳐 연결 및 프레임 크기 축소 ---③\n",
    "cap = cv2.VideoCapture(0)              \n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "\n",
    "while cap.isOpened():       \n",
    "    ret, frame = cap.read() \n",
    "    if img1 is None:  # 등록된 이미지 없음, 카메라 바이패스\n",
    "        res = frame\n",
    "    else:             # 등록된 이미지 있는 경우, 매칭 시작\n",
    "        img2 = frame\n",
    "        gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "        gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
    "        # 키포인트와 디스크립터 추출\n",
    "        kp1, desc1 = detector.detectAndCompute(gray1, None)\n",
    "        kp2, desc2 = detector.detectAndCompute(gray2, None)\n",
    "        # k=2로 knnMatch\n",
    "        matches = matcher.knnMatch(desc1, desc2, 2)\n",
    "        # 이웃 거리의 75%로 좋은 매칭점 추출---②\n",
    "        ratio = 0.75\n",
    "        good_matches = [m[0] for m in matches \\\n",
    "                if len(m) == 2 and m[0].distance < m[1].distance * ratio]\n",
    "        print('good matches:%d/%d' %(len(good_matches),len(matches)))\n",
    "        # 모든 매칭점 그리지 못하게 마스크를 0으로 채움\n",
    "        matchesMask = np.zeros(len(good_matches)).tolist()\n",
    "        # 좋은 매칭점 최소 갯수 이상 인 경우\n",
    "        if len(good_matches) > MIN_MATCH: \n",
    "            # 좋은 매칭점으로 원본과 대상 영상의 좌표 구하기 ---③\n",
    "            src_pts = np.float32([ kp1[m.queryIdx].pt for m in good_matches ])\n",
    "            dst_pts = np.float32([ kp2[m.trainIdx].pt for m in good_matches ])\n",
    "            # 원근 변환 행렬 구하기 ---⑤\n",
    "            mtrx, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "            accuracy=float(mask.sum()) / mask.size\n",
    "            print(\"accuracy: %d/%d(%.2f%%)\"% (mask.sum(), mask.size, accuracy))\n",
    "            if mask.sum() > MIN_MATCH:  # 정상치 매칭점 최소 갯수 이상 인 경우\n",
    "                # 이상점 매칭점만 그리게 마스크 설정\n",
    "                matchesMask = mask.ravel().tolist()\n",
    "                # 원본 영상 좌표로 원근 변환 후 영역 표시  ---⑦\n",
    "                h,w, = img1.shape[:2]\n",
    "                pts = np.float32([ [[0,0]],[[0,h-1]],[[w-1,h-1]],[[w-1,0]] ])\n",
    "                dst = cv2.perspectiveTransform(pts,mtrx)\n",
    "                img2 = cv2.polylines(img2,[np.int32(dst)],True,255,3, cv2.LINE_AA)\n",
    "        # 마스크로 매칭점 그리기 ---⑨\n",
    "        res = cv2.drawMatches(img1, kp1, img2, kp2, good_matches, None, \\\n",
    "                            matchesMask=matchesMask,\n",
    "                            flags=cv2.DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS)\n",
    "    # 결과 출력\n",
    "    cv2.imshow(win_name, res)\n",
    "    key = cv2.waitKey(1)\n",
    "    if key == 27:    # Esc, 종료\n",
    "            break          \n",
    "    elif key == ord(' '): # 스페이스바를 누르면 ROI로 img1 설정\n",
    "        x,y,w,h = cv2.selectROI(win_name, frame, False)\n",
    "        if w and h:\n",
    "            img1 = frame[y:y+h, x:x+w]\n",
    "else:\n",
    "    print(\"can't open camera.\")\n",
    "cap.release()                          \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 객체 추적"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 동영상배경제거\n",
    ": 배경이 있는 영상에서 객체가 있는 영상을 뺀다.\n",
    "* cv2.bgsegm.createBackgroundSubtractorMOG()\n",
    ": open cv에서 배경을 제거하는 다양한 알고리즘을 하나의 인터페이스로 통일한 배경객체 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BackgroundSubtractorMOG 배경제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, cv2\n",
    "\n",
    "cap = cv2.VideoCapture('img/walking.avi')\n",
    "fps = cap.get(cv2.CAP_PROP_FPS) # 프레임 수 구하기\n",
    "delay = int(1000/fps)\n",
    "# 배경 제거 객체 생성 --- ①\n",
    "fgbg = cv2.bgsegm.createBackgroundSubtractorMOG()\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    # 배경 제거 마스크 계산 --- ②\n",
    "    fgmask = fgbg.apply(frame)\n",
    "    cv2.imshow('frame',frame)\n",
    "    cv2.imshow('bgsub',fgmask)\n",
    "    if cv2.waitKey(1) & 0xff == 27:\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BackgroundSubtractorMOG2 배경제거\n",
    ": 가우시안 분포값을 사용, 그림자 검출 \\가능(느려짐)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, cv2\n",
    "\n",
    "cap = cv2.VideoCapture('img/walking.avi')\n",
    "fps = cap.get(cv2.CAP_PROP_FPS) # 프레임 수 구하기\n",
    "delay = int(1000/fps)\n",
    "# 배경 제거 객체 생성 --- ①\n",
    "fgbg = cv2.createBackgroundSubtractorMOG2()\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    # 배경 제거 마스크 계산 --- ②\n",
    "    fgmask = fgbg.apply(frame)\n",
    "    cv2.imshow('frame',frame)\n",
    "    cv2.imshow('bgsub',fgmask)\n",
    "    if cv2.waitKey(delay) & 0xff == 27:\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BackgroundSubtractorMOG2 배경제거_2 ( 팀원 노트북으로 같이 확인해보기)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(3.4.2) C:\\projects\\opencv-python\\opencv\\modules\\highgui\\src\\window.cpp:356: error: (-215:Assertion failed) size.width>0 && size.height>0 in function 'cv::imshow'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-3e4ddb2da1b0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m           \u001b[1;31m# 다음 프레임 읽기\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mfgmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfgbg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'camera'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfgmask\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m# 다음 프레임 이미지 표시\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwaitKey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# 1ms 동안 키 입력 대기 ---②\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;32mbreak\u001b[0m                   \u001b[1;31m# 아무 키라도 입력이 있으면 중지\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(3.4.2) C:\\projects\\opencv-python\\opencv\\modules\\highgui\\src\\window.cpp:356: error: (-215:Assertion failed) size.width>0 && size.height>0 in function 'cv::imshow'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture(0)               # 0번 카메라 장치 연결 ---①\n",
    "cap.set(3, 480)\n",
    "fgbg = cv2.createBackgroundSubtractorMOG2()\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()           # 다음 프레임 읽기\n",
    "    fgmask = fgbg.apply(frame)\n",
    "    cv2.imshow('camera', fgmask)   # 다음 프레임 이미지 표시\n",
    "    if cv2.waitKey(1) != -1:    # 1ms 동안 키 입력 대기 ---②\n",
    "        break                   # 아무 키라도 입력이 있으면 중지\n",
    "\n",
    "cap.release()                           # 자원 반납\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 옵티컬 플로\n",
    ": 이전 장면과 다음 장면 사이의 픽셀이 이동한 방향과 거리에 대한 분포\n",
    "\n",
    "* coners = cv2.goodFeaturesToTrack(img, maxCorners, qualitLevel, minDistance)\n",
    "    - img : 입력 영상\n",
    "    - maxCorners : 얻고 싶은 코너 개수 (강한 순)\n",
    "    - qualityLevel : 코너로 판단할 스레시 홀드 값\n",
    "    - minDistance : 코너간 최소거리\n",
    "    - cornes : 코너 검출 좌표 결과 (실수 값으로 정수로 변형 필요)\n",
    "####    \n",
    "* nextPt, status, err = cv2.calcOpticalFlowPyrLK(prevImg, nextImg, prevPt, nextPt \\\n",
    "  [criteria=termcriteria])\n",
    "    - prevImg : 이전 프레임 영상 \n",
    "    - nextImg : 다름 프레임 영상\n",
    "    - prevPt : 이전 프레임의 코너 특징점, cv2.goodFeaturesToTrack()으로 검출\n",
    "    - nextPt : 다음 프레임 에서 이동한 코너 특징점\n",
    "    - criteria=(COUNT + EPS, 30, 0.01) : 반복 탐색 중지 요건\n",
    "        - cv2.TERM_CRITERIA_EPS : 정확도가 epsilon보다 작으면\n",
    "        - cv2.TERM_CRITERIA_COUNT : max_iter(최대반복횟수)를 다 채우면\n",
    "    - status : 결과 상태 벡터 \n",
    "    - err : 결과 에러 벡터 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calcOpticalFlowPyrLK를 이용한 추적 \n",
    "    - Esc:종료\n",
    "    - Backspace:추적 이력 지우기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, cv2\n",
    "\n",
    "cap = cv2.VideoCapture('img/walking.avi')\n",
    "fps = cap.get(cv2.CAP_PROP_FPS) # 프레임 수 구하기\n",
    "delay = int(1000/fps)\n",
    "# 추적 경로를 그리기 위한 랜덤 색상\n",
    "color = np.random.randint(0,255,(200,3))\n",
    "lines = None  #추적 선을 그릴 이미지 저장 변수\n",
    "prevImg = None  # 이전 프레임 저장 변수\n",
    "# calcOpticalFlowPyrLK 중지 요건 설정\n",
    "# 1. cv2.TERM_CRITERIA_EPS 정확도가 epsilon보다 작으면\n",
    "#  2. cv2.TERM_CRITERIA_COUNT 최대반복횟수를 채우면\n",
    "termcriteria =  (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03)\n",
    "\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret,frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    img_draw = frame.copy()\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    # 최초 프레임 경우\n",
    "    if prevImg is None:\n",
    "        prevImg = gray\n",
    "        # 추적선 그릴 이미지를 프레임 크기에 맞게 생성\n",
    "        lines = np.zeros_like(frame)\n",
    "        # 추적 시작을 위한 코너 검출  ---①\n",
    "        prevPt = cv2.goodFeaturesToTrack(prevImg, 200, 0.01, 10)\n",
    "    else:\n",
    "        nextImg = gray\n",
    "        # 옵티컬 플로우로 다음 프레임의 코너점  찾기 ---②\n",
    "        nextPt, status, err = cv2.calcOpticalFlowPyrLK(prevImg, nextImg, \\\n",
    "                                        prevPt, None, criteria=termcriteria) # criteria : 반복탐색중지요건\n",
    "        # 대응점이 있는 코너, 움직인 코너 선별 ---③\n",
    "        prevMv = prevPt[status==1]\n",
    "        nextMv = nextPt[status==1]\n",
    "        for i,(p, n) in enumerate(zip(prevMv, nextMv)):\n",
    "            px,py = p.ravel() #배열을 펴준다.\n",
    "            nx,ny = n.ravel()\n",
    "            # 이전 코너와 새로운 코너에 선그리기 ---④\n",
    "            cv2.line(lines, (px, py), (nx,ny), color[i].tolist(), 2)\n",
    "            # 새로운 코너에 점 그리기\n",
    "            cv2.circle(img_draw, (nx,ny), 2, color[i].tolist(), -1)\n",
    "        # 누적된 추적 선을 출력 이미지에 합성 ---⑤\n",
    "        img_draw = cv2.add(img_draw, lines)\n",
    "        # 다음 프레임을 위한 프레임과 코너점 이월\n",
    "        prevImg = nextImg\n",
    "        prevPt = nextMv.reshape(-1,1,2)\n",
    "\n",
    "    cv2.imshow('OpticalFlow-LK', img_draw)\n",
    "    key = cv2.waitKey(delay)\n",
    "    if key == 27 : # Esc:종료\n",
    "        break\n",
    "    elif key == 8: # Backspace:추적 이력 지우기\n",
    "        prevImg = None\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CamShift 추적\n",
    "    - 코드 설명 생략\n",
    "    - 아래의 TrackAPI와 같이 팀원 노트북으로 같이 확인해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, cv2\n",
    "\n",
    "roi_hist = None     # 추적 객체 히스토그램 저장 변수\n",
    "win_name = 'Camshift Tracking'\n",
    "termination =  (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 1)\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()    \n",
    "    img_draw = frame.copy()\n",
    "    \n",
    "    if roi_hist is not None:  # 추적 대상 객체 히스토그램 등록 됨\n",
    "        # 전체 영상 hsv 컬로 변환 ---①\n",
    "        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "        # 전체 영상 히스토그램과 roi 히스트그램 역투영 ---②\n",
    "        dst = cv2.calcBackProject([hsv], [0], roi_hist, [0,180], 1)\n",
    "        # 역 투영 결과와 초기 추적 위치로 평균 이동 추적 ---③\n",
    "        ret, (x,y,w,h) = cv2.CamShift(dst, (x,y,w,h), termination)\n",
    "        # 새로운 위치에 사각형 표시 ---④\n",
    "        cv2.rectangle(img_draw, (x,y), (x+w, y+h), (0,255,0), 2)\n",
    "        # 컬러 영상과 역투영 영상을 통합해서 출력\n",
    "        result = np.hstack((img_draw, cv2.cvtColor(dst, cv2.COLOR_GRAY2BGR)))\n",
    "    else:  # 추적 대상 객체 히스토그램 등록 안됨\n",
    "        cv2.putText(img_draw, \"Hit the Space to set target to track\", \\\n",
    "                (10,30),cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 1, cv2.LINE_AA)\n",
    "        result = img_draw\n",
    "\n",
    "    cv2.imshow(win_name, result)\n",
    "    key = cv2.waitKey(1) & 0xff\n",
    "    if  key == 27: # Esc\n",
    "        break\n",
    "    elif key == ord(' '): # 스페이스-바, ROI 설정\n",
    "        x,y,w,h = cv2.selectROI(win_name, frame, False)\n",
    "        if w and h :    # ROI가 제대로 설정됨\n",
    "            # 초기 추적 대상 위치로 roi 설정 --- ⑤\n",
    "            roi = frame[y:y+h, x:x+w]\n",
    "            # roi를 HSV 컬러로 변경 ---⑥\n",
    "            roi = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)\n",
    "            mask = None\n",
    "            # roi에 대한 히스토그램 계산 ---⑦\n",
    "            roi_hist = cv2.calcHist([roi], [0], mask, [180], [0,180])\n",
    "            cv2.normalize(roi_hist, roi_hist, 0, 255, cv2.NORM_MINMAX)\n",
    "        else:                       # ROI 설정 안됨\n",
    "            roi_hist = None\n",
    "else:\n",
    "    print('no camera!')\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracking API \n",
    "    - 17번째 주석 풀고, 16번째 주석처리하여 결과 확인\n",
    "    - 조원 노트북으로 확인하기\n",
    "    - 0 ~ 7 숫자 ( 5제외, OPEN CV 3.4버전에서 버그) 변경해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-616ae16d490a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mcap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVideoCapture\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvideo_src\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mfps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCAP_PROP_FPS\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 프레임 수 구하기\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mdelay\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mfps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[0mwin_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Tracking APIs'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[0mcap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misOpened\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# 트랙커 객체 생성자 함수 리스트 ---①\n",
    "trackers = [cv2.TrackerBoosting_create,\n",
    "            cv2.TrackerMIL_create,\n",
    "            cv2.TrackerKCF_create,\n",
    "            cv2.TrackerTLD_create,\n",
    "            cv2.TrackerMedianFlow_create,\n",
    "            cv2.TrackerGOTURN_create, #버그로 오류 발생\n",
    "            cv2.TrackerCSRT_create,\n",
    "            cv2.TrackerMOSSE_create]\n",
    "trackerIdx = 0  # 트랙커 생성자 함수 선택 인덱스\n",
    "tracker = None\n",
    "isFirst = True\n",
    "\n",
    "video_src = 0 # 비디오 파일과 카메라 선택 ---②\n",
    "#video_src = \"img/highway.mp4\"\n",
    "cap = cv2.VideoCapture(video_src)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS) # 프레임 수 구하기\n",
    "delay = int(1000/fps)\n",
    "win_name = 'Tracking APIs'\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print('Cannot read video file')\n",
    "        break\n",
    "    img_draw = frame.copy()\n",
    "    if tracker is None: # 트랙커 생성 안된 경우\n",
    "        cv2.putText(img_draw, \"Press the Space to set ROI!!\", \\\n",
    "            (100,80), cv2.FONT_HERSHEY_SIMPLEX, 0.75,(0,0,255),2,cv2.LINE_AA)\n",
    "    else:\n",
    "        ok, bbox = tracker.update(frame)   # 새로운 프레임에서 추적 위치 찾기 ---③\n",
    "        (x,y,w,h) = bbox\n",
    "        if ok: # 추적 성공\n",
    "            cv2.rectangle(img_draw, (int(x), int(y)), (int(x + w), int(y + h)), \\\n",
    "                          (0,255,0), 2, 1)\n",
    "        else : # 추적 실패\n",
    "            cv2.putText(img_draw, \"Tracking fail.\", (100,80), \\\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.75,(0,0,255),2,cv2.LINE_AA)\n",
    "    trackerName = tracker.__class__.__name__\n",
    "    cv2.putText(img_draw, str(trackerIdx) + \":\"+trackerName , (100,20), \\\n",
    "                 cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0,255,0),2,cv2.LINE_AA)\n",
    "\n",
    "    cv2.imshow(win_name, img_draw)\n",
    "    key = cv2.waitKey(delay) & 0xff\n",
    "    # 스페이스 바 또는 비디오 파일 최초 실행 ---④\n",
    "    if key == ord(' ') or (video_src != 0 and isFirst): \n",
    "        isFirst = False\n",
    "        roi = cv2.selectROI(win_name, frame, False)  # 초기 객체 위치 설정\n",
    "        if roi[2] and roi[3]:         # 위치 설정 값 있는 경우\n",
    "            tracker = trackers[trackerIdx]()    #트랙커 객체 생성 ---⑤\n",
    "            isInit = tracker.init(frame, roi)\n",
    "    elif key in range(48, 56): # 0~7 숫자 입력   ---⑥\n",
    "        trackerIdx = key-48     # 선택한 숫자로 트랙커 인덱스 수정\n",
    "        if bbox is not None:\n",
    "            tracker = trackers[trackerIdx]() # 선택한 숫자의 트랙커 객체 생성 ---⑦\n",
    "            isInit = tracker.init(frame, bbox) # 이전 추적 위치로 추적 위치 초기화\n",
    "    elif key == 27 : \n",
    "        break\n",
    "else:\n",
    "    print( \"Could not open video\")\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 책표지 검색기\n",
    "    - 코드 설명 생략, 조원 컴퓨터로 확인해보기\n",
    "    - 책을 사각형안에 넣고 Space Bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Camera!!\n"
     ]
    }
   ],
   "source": [
    "import cv2 , glob, numpy as np\n",
    "\n",
    "# 검색 설정 변수 ---①\n",
    "ratio = 0.7\n",
    "MIN_MATCH = 10\n",
    "# ORB 특징 검출기 생성 ---②\n",
    "detector = cv2.ORB_create()\n",
    "# Flann 매칭기 객체 생성 ---③\n",
    "FLANN_INDEX_LSH = 6\n",
    "index_params= dict(algorithm = FLANN_INDEX_LSH,\n",
    "                   table_number = 6,\n",
    "                   key_size = 12,\n",
    "                   multi_probe_level = 1)\n",
    "search_params=dict(checks=32)\n",
    "matcher = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "\n",
    "# 책 표지 검색 함수 ---④\n",
    "def serch(img):\n",
    "    gray1 = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    kp1, desc1 = detector.detectAndCompute(gray1, None)\n",
    "    \n",
    "    results = {}\n",
    "    # 책 커버 보관 디렉토리 경로 ---⑤\n",
    "    cover_paths = glob.glob('img/books/*.*')\n",
    "    for cover_path in cover_paths:\n",
    "        cover = cv2.imread(cover_path)\n",
    "        cv2.imshow('Searching...', cover) # 검색 중인 책 표지 표시 ---⑥\n",
    "        cv2.waitKey(5)\n",
    "        gray2 = cv2.cvtColor(cover, cv2.COLOR_BGR2GRAY)\n",
    "        kp2, desc2 = detector.detectAndCompute(gray2, None) # 특징점 검출 ---⑦\n",
    "        matches = matcher.knnMatch(desc1, desc2, 2) # 특징점 매칭 ---⑧\n",
    "        # 좋은 매칭 선별 ---⑨\n",
    "        good_matches = [m[0] for m in matches \\\n",
    "                    if len(m) == 2 and m[0].distance < m[1].distance * ratio]\n",
    "        if len(good_matches) > MIN_MATCH: \n",
    "            # 좋은 매칭점으로 원본과 대상 영상의 좌표 구하기 ---⑩\n",
    "            src_pts = np.float32([ kp1[m.queryIdx].pt for m in good_matches ])\n",
    "            dst_pts = np.float32([ kp2[m.trainIdx].pt for m in good_matches ])\n",
    "            # 원근 변환 행렬 구하기 ---⑪\n",
    "            mtrx, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "            # 원근 변환 결과에서 정상치 비율 계산 ---⑫\n",
    "            accuracy=float(mask.sum()) / mask.size\n",
    "            results[cover_path] = accuracy\n",
    "    cv2.destroyWindow('Searching...')\n",
    "    if len(results) > 0:\n",
    "        results = sorted([(v,k) for (k,v) in results.items() \\\n",
    "                    if v > 0], reverse=True)\n",
    "    return results\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "qImg = None\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print('No Frame!')\n",
    "        break\n",
    "    h, w = frame.shape[:2]\n",
    "    # 화면에 책을 인식할 영역 표시 ---⑬\n",
    "    left = w // 3\n",
    "    right = (w // 3) * 2\n",
    "    top = (h // 2) - (h // 3)\n",
    "    bottom = (h // 2) + (h // 3)\n",
    "    cv2.rectangle(frame, (left,top), (right,bottom), (255,255,255), 3)\n",
    "    \n",
    "    # 거울 처럼 보기 좋게 화면 뒤집어 보이기\n",
    "    flip = cv2.flip(frame,1)\n",
    "    cv2.imshow('Book Searcher', flip)\n",
    "    key = cv2.waitKey(10)\n",
    "    if key == ord(' '): # 스페이스-바를 눌러서 사진 찍기\n",
    "        qImg = frame[top:bottom , left:right]\n",
    "        cv2.imshow('query', qImg)\n",
    "        break\n",
    "    elif key == 27 : #Esc\n",
    "        break\n",
    "else:\n",
    "    print('No Camera!!')\n",
    "cap.release()\n",
    "\n",
    "if qImg is not None:\n",
    "    gray = cv2.cvtColor(qImg, cv2.COLOR_BGR2GRAY)\n",
    "    results = serch(qImg)\n",
    "    if len(results) == 0 :\n",
    "        print(\"No matched book cover found.\")\n",
    "    else:\n",
    "        for( i, (accuracy, cover_path)) in enumerate(results):\n",
    "            print(i, cover_path, accuracy)\n",
    "            if i==0:\n",
    "                cover = cv2.imread(cover_path)\n",
    "                cv2.putText(cover, (\"Accuracy:%.2f%%\"%(accuracy*100)), (10,100), \\\n",
    "                             cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2, cv2.LINE_AA)\n",
    "    cv2.imshow('Result', cover)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
